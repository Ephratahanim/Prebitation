{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Long, Short or Stable position based on some indicators computed from historical price values.\n",
    "\n",
    "- State : Features + Closing price\n",
    "- Action : Pick Short, Long or Stable position\n",
    "- Reward : (p1-p2)*position\n",
    "\n",
    "shortfalls:\n",
    "- No temporal modeling\n",
    "- No portfolia management\n",
    "- Very basic reward function\n",
    "- Avg. Training reward flactuates due to intantenous rate of return differences used in the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      " > There are 24318 rows\n",
      "(23118, 8)\n",
      "(1200, 8)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# np.random.seed(1335)  # for reproducibility\n",
    "np.set_printoptions(precision=5, suppress=True, linewidth=150)\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import reinforcement_learning.backtest as twp\n",
    "from reinforcement_learning.utils import *\n",
    "\n",
    "DATA_PATH = \"../data/bitcoin-historical-data/coinbaseUSD_1-min_data_2014-12-01_to_2017-10-20.csv.csv\"\n",
    "TIME_GRAN = 60  # time granularity in minutes\n",
    "TRAIN_START = 0 / TIME_GRAN  # take only afterwards\n",
    "PRED_DAYS = 50  # num prediction days for the trained models\n",
    "TEST_START = ((24*60) / TIME_GRAN) * PRED_DAYS \n",
    "NUM_ACTIONS = 3\n",
    "\n",
    "df = read_data(DATA_PATH, TIME_GRAN)\n",
    "df_train = df.iloc[TRAIN_START:df.shape[0]-TEST_START,] \n",
    "df_test = df.iloc[df.shape[0]-TEST_START:,]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"lecun_uniform\", input_shape=(7,))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"lecun_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "num_features = 7\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, init='lecun_uniform', input_shape=(num_features,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32, init='lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(NUM_ACTIONS, init='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "rms = RMSprop()\n",
    "adam = Adam()\n",
    "# sgd = SGD(lr=0.01, momentum=0.5)\n",
    "# model.compile(loss='mse', optimizer=adam)\n",
    "model.compile(loss=huber_loss, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action: 2, Signal: -1, Reward: -0.0 Price: 370.0 TimeStep: 14\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 370.0 TimeStep: 15\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 370.0 TimeStep: 16\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 370.0 TimeStep: 17\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 370.0 TimeStep: 18\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 370.0 TimeStep: 19\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 370.0 TimeStep: 20\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 370.0 TimeStep: 21\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 370.0 TimeStep: 22\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 377.0 TimeStep: 23\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 377.0 TimeStep: 24\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 377.0 TimeStep: 25\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 377.0 TimeStep: 26\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 377.0 TimeStep: 27\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 377.0 TimeStep: 28\n",
      " Action: 2, Signal: -1, Reward: -0.0026525198939 Price: 378.0 TimeStep: 29\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 30\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 31\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 32\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 33\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 34\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 35\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 36\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 37\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 38\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 39\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 40\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 41\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 42\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 43\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 44\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 45\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 46\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 47\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 48\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 49\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 50\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 51\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 52\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 53\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 54\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 55\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 56\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 57\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 58\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 59\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 378.0 TimeStep: 60\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 61\n",
      " Action: 1, Signal: 1, Reward: -0.00261902178406 Price: 377.010009766 TimeStep: 62\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 377.010009766 TimeStep: 63\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 377.010009766 TimeStep: 64\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 378.0 TimeStep: 65\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 66\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 378.0 TimeStep: 67\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 337.899993896 TimeStep: 68\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 275.670013428 TimeStep: 69\n",
      " Action: 1, Signal: 1, Reward: 0.0652954587849 Price: 293.670013428 TimeStep: 70\n",
      " Action: 2, Signal: -1, Reward: -0.0202266886377 Price: 299.609985352 TimeStep: 71\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 282.850006104 TimeStep: 72\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 310.309997559 TimeStep: 73\n",
      " Action: 2, Signal: -1, Reward: 0.0010312504487 Price: 309.989990234 TimeStep: 74\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 289.25 TimeStep: 75\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 289.25 TimeStep: 76\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 289.25 TimeStep: 77\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 289.25 TimeStep: 78\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 289.25 TimeStep: 79\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 289.25 TimeStep: 80\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 289.25 TimeStep: 81\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 289.25 TimeStep: 82\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 289.25 TimeStep: 83\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 289.25 TimeStep: 84\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 289.25 TimeStep: 85\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 289.25 TimeStep: 86\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 260.0 TimeStep: 87\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 88\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 260.0 TimeStep: 89\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 90\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 91\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 260.0 TimeStep: 92\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 93\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 94\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 95\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 96\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 97\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 98\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 99\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 100\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 260.0 TimeStep: 101\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 102\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 260.0 TimeStep: 103\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 260.0 TimeStep: 104\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 260.0 TimeStep: 105\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 260.0 TimeStep: 106\n",
      " Action: 2, Signal: -1, Reward: 0.230769230769 Price: 200.0 TimeStep: 107\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 200.0 TimeStep: 108\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 200.0 TimeStep: 109\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 200.0 TimeStep: 110\n",
      " Action: 1, Signal: 1, Reward: 0.0981500244141 Price: 219.630004883 TimeStep: 111\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 219.630004883 TimeStep: 112\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 219.630004883 TimeStep: 113\n",
      " Action: 1, Signal: 1, Reward: -0.00910622390173 Price: 217.630004883 TimeStep: 114\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 199.0 TimeStep: 115\n",
      " Action: 1, Signal: 1, Reward: -0.431859308751 Price: 113.059997559 TimeStep: 116\n",
      " Action: 2, Signal: -1, Reward: -0.329205749376 Price: 150.279998779 TimeStep: 117\n",
      " Action: 1, Signal: 1, Reward: 0.290191648686 Price: 193.88999939 TimeStep: 118\n",
      " Action: 1, Signal: 1, Reward: 0.00593116664142 Price: 195.039993286 TimeStep: 119\n",
      " Action: 2, Signal: -1, Reward: 0.350287091967 Price: 126.720001221 TimeStep: 120\n",
      " Action: 2, Signal: -1, Reward: -0.412641988498 Price: 179.009994507 TimeStep: 121\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 166.059997559 TimeStep: 122\n",
      " Action: 1, Signal: 1, Reward: 0.182223354273 Price: 196.320007324 TimeStep: 123\n",
      " Action: 2, Signal: -1, Reward: 0.0530256890472 Price: 185.910003662 TimeStep: 124\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 185.910003662 TimeStep: 125\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 186.0 TimeStep: 126\n",
      " Action: 1, Signal: 1, Reward: -0.354838709677 Price: 120.0 TimeStep: 127\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 120.0 TimeStep: 128\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 120.0 TimeStep: 129\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 120.0 TimeStep: 130\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 120.0 TimeStep: 131\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 120.0 TimeStep: 132\n",
      " Action: 2, Signal: -1, Reward: -0.6 Price: 192.0 TimeStep: 133\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 192.0 TimeStep: 134\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 192.0 TimeStep: 135\n",
      " Action: 2, Signal: -1, Reward: 0.21875 Price: 150.0 TimeStep: 136\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 150.0 TimeStep: 137\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 150.0 TimeStep: 138\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 139\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 150.0 TimeStep: 140\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 150.0 TimeStep: 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Action: 1, Signal: 1, Reward: 0.0 Price: 150.0 TimeStep: 142\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 150.0 TimeStep: 143\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 150.0 TimeStep: 144\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 145\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 146\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 147\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 150.0 TimeStep: 148\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 149\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 150.0 TimeStep: 150\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 150.0 TimeStep: 151\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 224.0 TimeStep: 152\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 224.0 TimeStep: 153\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 209.419998169 TimeStep: 154\n",
      " Action: 1, Signal: 1, Reward: -0.0071626397341 Price: 207.919998169 TimeStep: 155\n",
      " Action: 2, Signal: -1, Reward: 0.0167853286067 Price: 204.429992676 TimeStep: 156\n",
      " Action: 2, Signal: -1, Reward: 0.00699502385665 Price: 203.0 TimeStep: 157\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 205.0 TimeStep: 158\n",
      " Action: 0, Signal: 0, Reward: -0.0 Price: 189.0 TimeStep: 159\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 160\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 161\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 189.0 TimeStep: 162\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 189.0 TimeStep: 163\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 164\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 165\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 166\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 167\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 168\n",
      " Action: 0, Signal: 0, Reward: 0.0 Price: 189.0 TimeStep: 169\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 189.0 TimeStep: 170\n",
      " Action: 2, Signal: -1, Reward: -0.0 Price: 189.0 TimeStep: 171\n",
      " Action: 1, Signal: 1, Reward: 0.0 Price: 189.0 TimeStep: 172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e20c981e0b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Experience replay storage, if we add enough new experiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0moptimize_DQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# One step ahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/eightbit/data_hdd/Projects/Prebition/Notebooks/reinforcement_learning/utils.pyc\u001b[0m in \u001b[0;36moptimize_DQN\u001b[0;34m(model, replay, batch_size, gamma)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# print(y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# print(\"--\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# set train and test data\n",
    "train_data = df_train\n",
    "test_data = df_test\n",
    "start_timestep = 14\n",
    "\n",
    "# training arguments\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_sim = 924 \n",
    "gamma = 0\n",
    "epsilon = 0.99  # parameter setting initial random actions, that decays as learning \n",
    "\n",
    "# stores tuples of (S, A, R, S')\n",
    "signal = pd.Series(index=np.arange(len(train_data)), data=[None]*len(train_data))\n",
    "\n",
    "# replay memory\n",
    "replay_memory = ReplayMemory(10000)\n",
    "\n",
    "# let the agent learn\n",
    "learning_progress = []\n",
    "sim_count = 0\n",
    "for i in range(epochs):\n",
    "    avg_sim_reward = AverageMeter()\n",
    "    terminal_state = 0\n",
    "    states, price_data = create_states(train_data)\n",
    "    time_step = start_timestep\n",
    "    state = states[time_step][None, :]\n",
    "        \n",
    "    #while game still in progress\n",
    "    while(terminal_state == 0):\n",
    "        \n",
    "        # We are in state S\n",
    "        # 1- Compute the next action based on state S with the network\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,NUM_ACTIONS) #assumes 4 different actions\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            qval = model.predict(state, batch_size=1)\n",
    "            action = (np.argmax(qval))\n",
    "            \n",
    "        # 2- Take action, observe new state S'\n",
    "        new_state, new_time_step, signal, terminal_state = take_action(states, action, signal, time_step)\n",
    "        assert new_time_step - time_step == 1\n",
    "       \n",
    "        # 3- Compute the reward\n",
    "        reward = get_pos_reward(new_state, new_time_step, action, price_data, signal, eval=False)\n",
    "#         print(\" Action: {}, Signal: {}, Reward: {} Price: {} TimeStep: {}\".format(action, \n",
    "#                                                                                   signal[time_step],\n",
    "#                                                                                   reward,\n",
    "#                                                                                   price_data[new_time_step],\n",
    "#                                                                                   time_step))\n",
    "        \n",
    "        # Update average simulation reward\n",
    "        avg_sim_reward.update(reward)\n",
    "       \n",
    "        # Add new experience to replay memory\n",
    "        replay_memory.push(state, action, new_state, reward)\n",
    "        \n",
    "        # Experience replay storage, if we add enough new experiences\n",
    "        optimize_DQN(model, replay_memory, batch_size, gamma)\n",
    "        \n",
    "        # One step ahead\n",
    "        state = new_state\n",
    "        time_step = new_time_step\n",
    "        assert (states[new_time_step] - new_state).sum() == 0\n",
    "        \n",
    "    # eval model on test data\n",
    "    test_reward, action_count = evaluate_Q(test_data, model, get_pos_reward, )\n",
    "    print(action_count)\n",
    "    # learning_progress.append((eval_reward))\n",
    "    learning_progress.append(avg_sim_reward.avg)\n",
    "    print(\"Epoch #: %s SimReward: %f Epsilon: %f TestReward: %f\" % (i, avg_sim_reward.avg, epsilon, test_reward))\n",
    "    \n",
    "    # decay epsilon\n",
    "    if epsilon > 0.1: #decrement epsilon over time\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "elapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, action_count = evaluate_Q(test_data, model, get_pos_reward, )\n",
    "print(reward)\n",
    "print(action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, price_data = create_states(test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal = pd.Series(index=np.arange(len(test_data)))\n",
    "state, xdata, price_data = create_states(test_data, test=True)\n",
    "\n",
    "time_step = 1\n",
    "terminal_state = 0\n",
    "avg_test_reward = AverageMeter()\n",
    "while(terminal_state == 0):\n",
    "    #We are in state S\n",
    "    # 1- Compute the next action based on state S with the network\n",
    "    qval = model.predict(state, batch_size=1)\n",
    "    print(qval)\n",
    "    action = (np.argmax(qval))\n",
    "    # 2- Take action, observe new state S'\n",
    "    new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        \n",
    "    # 3- Compute the reward\n",
    "#     reward = get_reward(new_state, time_step, action, close_prices, signal, eval=False)\n",
    "#     avg_test_reward.update(reward)\n",
    "    state = new_state\n",
    "#     print(\" # Average test reward: {}\".format(avg_test_reward.avg))\n",
    "    \n",
    "final_reward = get_pos_reward(new_state, time_step, action, price_data, signal, eval=True)\n",
    "print(\" # Final test reward: {}\".format(final_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_data = pd.Series(price_data)\n",
    "bt = twp.Backtest(price_data, signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "unique, counts = np.unique(filter(lambda v: v==v, signal.values), return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 40])\n",
    "plt.subplot(3,1,1)\n",
    "bt.plotTrades()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_progress)\n",
    "\n",
    "plt.savefig('reinforcement_learning/plt/summary'+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "894px",
    "left": "1670px",
    "right": "20.2333px",
    "top": "121px",
    "width": "454px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
