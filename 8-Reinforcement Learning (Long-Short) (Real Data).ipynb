{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Long, Short or Stable position based on some indicators computed from historical price values.\n",
    "\n",
    "- State : Features + Closing price\n",
    "- Action : Pick Short, Long or Stable position\n",
    "- Reward : (p1-p2)*position\n",
    "\n",
    "shortfalls:\n",
    "- No temporal modeling\n",
    "- No portfolia management\n",
    "- Very basic reward function\n",
    "- Avg. Training reward flactuates due to intantenous rate of return differences used in the reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      " > There are 24318 rows\n",
      "(23118, 8)\n",
      "(1200, 8)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# np.random.seed(1335)  # for reproducibility\n",
    "np.set_printoptions(precision=5, suppress=True, linewidth=150)\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import reinforcement_learning.backtest as twp\n",
    "from reinforcement_learning.utils import *\n",
    "\n",
    "DATA_PATH = \"../data/bitcoin-historical-data/coinbaseUSD_1-min_data_2014-12-01_to_2017-10-20.csv.csv\"\n",
    "TIME_GRAN = 60  # time granularity in minutes\n",
    "TRAIN_START = 0 / TIME_GRAN  # take only afterwards\n",
    "PRED_DAYS = 50  # num prediction days for the trained models\n",
    "TEST_START = ((24*60) / TIME_GRAN) * PRED_DAYS \n",
    "NUM_ACTIONS = 3\n",
    "\n",
    "df = read_data(DATA_PATH, TIME_GRAN)\n",
    "df_train = df.iloc[TRAIN_START:df.shape[0]-TEST_START,] \n",
    "df_test = df.iloc[df.shape[0]-TEST_START:,]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"lecun_uniform\", input_shape=(7,))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"lecun_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "num_features = 7\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, init='lecun_uniform', input_shape=(num_features,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32, init='lecun_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(NUM_ACTIONS, init='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "rms = RMSprop()\n",
    "adam = Adam()\n",
    "# sgd = SGD(lr=0.01, momentum=0.5)\n",
    "# model.compile(loss='mse', optimizer=adam)\n",
    "model.compile(loss=huber_loss, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   -1.  1188.]\n",
      " [    0.     4.]\n",
      " [    1.     8.]]\n",
      "Epoch #: 0 SimReward: -0.000085 Epsilon: 0.990000 TestReward: -1038.580078\n",
      "[[   -1.     5.]\n",
      " [    0.     1.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 1 SimReward: 0.000057 Epsilon: 0.980000 TestReward: 1102.100098\n",
      "[[   -1.     4.]\n",
      " [    0.  1196.]]\n",
      "Epoch #: 2 SimReward: 0.000032 Epsilon: 0.970000 TestReward: -24.850098\n",
      "[[   -1.     5.]\n",
      " [    0.     8.]\n",
      " [    1.  1187.]]\n",
      "Epoch #: 3 SimReward: 0.000012 Epsilon: 0.960000 TestReward: 1274.000488\n",
      "[[   -1.     5.]\n",
      " [    0.     1.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 4 SimReward: 0.000050 Epsilon: 0.950000 TestReward: 1102.100098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 5 SimReward: 0.000015 Epsilon: 0.940000 TestReward: -1131.199707\n",
      "[[   -1.     5.]\n",
      " [    0.  1195.]]\n",
      "Epoch #: 6 SimReward: -0.000048 Epsilon: 0.930000 TestReward: -14.549805\n",
      "[[   -1.     8.]\n",
      " [    0.     1.]\n",
      " [    1.  1191.]]\n",
      "Epoch #: 7 SimReward: 0.000026 Epsilon: 0.920000 TestReward: 1064.399902\n",
      "[[   -1.     3.]\n",
      " [    0.     2.]\n",
      " [    1.  1195.]]\n",
      "Epoch #: 8 SimReward: -0.000000 Epsilon: 0.910000 TestReward: 1081.549805\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 9 SimReward: 0.000039 Epsilon: 0.900000 TestReward: 1081.600098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 10 SimReward: -0.000070 Epsilon: 0.890000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 11 SimReward: 0.000016 Epsilon: 0.880000 TestReward: 1131.199707\n",
      "[[   -1.     5.]\n",
      " [    0.  1195.]]\n",
      "Epoch #: 12 SimReward: 0.000059 Epsilon: 0.870000 TestReward: -14.549805\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 13 SimReward: -0.000065 Epsilon: 0.860000 TestReward: 1081.600098\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 14 SimReward: 0.000022 Epsilon: 0.850000 TestReward: 1081.600098\n",
      "[[   -1.     5.]\n",
      " [    0.  1195.]]\n",
      "Epoch #: 15 SimReward: 0.000031 Epsilon: 0.840000 TestReward: -14.549805\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 16 SimReward: 0.000044 Epsilon: 0.830000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 17 SimReward: -0.000004 Epsilon: 0.820000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 18 SimReward: -0.000122 Epsilon: 0.810000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 19 SimReward: -0.000123 Epsilon: 0.800000 TestReward: -1131.199707\n",
      "[[   -1.     5.]\n",
      " [    0.     1.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 20 SimReward: -0.000035 Epsilon: 0.790000 TestReward: 1102.100098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 21 SimReward: 0.000042 Epsilon: 0.780000 TestReward: -1131.199707\n",
      "[[    0.     2.]\n",
      " [    1.  1198.]]\n",
      "Epoch #: 22 SimReward: -0.000017 Epsilon: 0.770000 TestReward: 1139.680176\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 23 SimReward: 0.000078 Epsilon: 0.760000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 24 SimReward: -0.000075 Epsilon: 0.750000 TestReward: 0.000000\n",
      "[[    0.     2.]\n",
      " [    1.  1198.]]\n",
      "Epoch #: 25 SimReward: 0.000064 Epsilon: 0.740000 TestReward: 1139.680176\n",
      "[[   -1.     1.]\n",
      " [    0.     1.]\n",
      " [    1.  1198.]]\n",
      "Epoch #: 26 SimReward: -0.000029 Epsilon: 0.730000 TestReward: 1148.160645\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 27 SimReward: 0.000122 Epsilon: 0.720000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 28 SimReward: 0.000004 Epsilon: 0.710000 TestReward: 1131.199707\n",
      "[[   -1.     1.]\n",
      " [    0.     3.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 29 SimReward: 0.000001 Epsilon: 0.700000 TestReward: 1114.880371\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 30 SimReward: -0.000026 Epsilon: 0.690000 TestReward: 1131.199707\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 31 SimReward: 0.000017 Epsilon: 0.680000 TestReward: 1081.600098\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 32 SimReward: 0.000041 Epsilon: 0.670000 TestReward: 1081.600098\n",
      "[[   -1.     3.]\n",
      " [    0.  1197.]]\n",
      "Epoch #: 33 SimReward: 0.000007 Epsilon: 0.660000 TestReward: -24.799805\n",
      "[[   -1.     8.]\n",
      " [    0.  1192.]]\n",
      "Epoch #: 34 SimReward: 0.000163 Epsilon: 0.650000 TestReward: -33.399902\n",
      "[[   -1.     3.]\n",
      " [    0.     6.]\n",
      " [    1.  1191.]]\n",
      "Epoch #: 35 SimReward: 0.000063 Epsilon: 0.640000 TestReward: 1073.000000\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 36 SimReward: 0.000056 Epsilon: 0.630000 TestReward: -1131.199707\n",
      "[[   -1.     3.]\n",
      " [    0.  1197.]]\n",
      "Epoch #: 37 SimReward: -0.000101 Epsilon: 0.620000 TestReward: -24.799805\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 38 SimReward: 0.000013 Epsilon: 0.610000 TestReward: -1131.199707\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 39 SimReward: 0.000012 Epsilon: 0.600000 TestReward: 1081.600098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 40 SimReward: 0.000039 Epsilon: 0.590000 TestReward: -1131.199707\n",
      "[[    0.     6.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 41 SimReward: -0.000008 Epsilon: 0.580000 TestReward: 1116.649902\n",
      "[[    0.     5.]\n",
      " [    1.  1195.]]\n",
      "Epoch #: 42 SimReward: -0.000033 Epsilon: 0.570000 TestReward: 1106.349609\n",
      "[[   -1.     5.]\n",
      " [    0.     1.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 43 SimReward: 0.000010 Epsilon: 0.560000 TestReward: 1102.100098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 44 SimReward: 0.000233 Epsilon: 0.550000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 45 SimReward: -0.000082 Epsilon: 0.540000 TestReward: 0.000000\n",
      "[[   -1.     3.]\n",
      " [    0.  1197.]]\n",
      "Epoch #: 46 SimReward: -0.000034 Epsilon: 0.530000 TestReward: -24.799805\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 47 SimReward: 0.000097 Epsilon: 0.520000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 48 SimReward: 0.000018 Epsilon: 0.510000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 49 SimReward: -0.000022 Epsilon: 0.500000 TestReward: 0.000000\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 50 SimReward: 0.000122 Epsilon: 0.490000 TestReward: 1081.600098\n",
      "[[   -1.     3.]\n",
      " [    0.     1.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 51 SimReward: 0.000046 Epsilon: 0.480000 TestReward: 1081.600098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 52 SimReward: 0.000076 Epsilon: 0.470000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 53 SimReward: 0.000074 Epsilon: 0.460000 TestReward: -1131.199707\n",
      "[[   -1.     4.]\n",
      " [    0.     1.]\n",
      " [    1.  1195.]]\n",
      "Epoch #: 54 SimReward: 0.000079 Epsilon: 0.450000 TestReward: 1081.499512\n",
      "[[    0.     4.]\n",
      " [    1.  1196.]]\n",
      "Epoch #: 55 SimReward: 0.000136 Epsilon: 0.440000 TestReward: 1106.399902\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 56 SimReward: -0.000048 Epsilon: 0.430000 TestReward: -1131.199707\n",
      "[[   -1.     5.]\n",
      " [    0.     1.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 57 SimReward: 0.000011 Epsilon: 0.420000 TestReward: 1102.100098\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 58 SimReward: -0.000132 Epsilon: 0.410000 TestReward: -1131.199707\n",
      "[[   -1.     3.]\n",
      " [    0.     3.]\n",
      " [    1.  1194.]]\n",
      "Epoch #: 59 SimReward: -0.000005 Epsilon: 0.400000 TestReward: 1091.850098\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 60 SimReward: -0.000001 Epsilon: 0.390000 TestReward: 0.000000\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 61 SimReward: 0.000054 Epsilon: 0.380000 TestReward: 1131.199707\n",
      "[[   -1.     1.]\n",
      " [    0.     1.]\n",
      " [    1.  1198.]]\n",
      "Epoch #: 62 SimReward: 0.000033 Epsilon: 0.370000 TestReward: 1148.160645\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 63 SimReward: 0.000072 Epsilon: 0.360000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 64 SimReward: 0.000036 Epsilon: 0.350000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 65 SimReward: -0.000030 Epsilon: 0.340000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 66 SimReward: -0.000015 Epsilon: 0.330000 TestReward: 0.000000\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 67 SimReward: -0.000001 Epsilon: 0.320000 TestReward: 0.000000\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 68 SimReward: -0.000056 Epsilon: 0.310000 TestReward: 0.000000\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 69 SimReward: 0.000009 Epsilon: 0.300000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 70 SimReward: -0.000084 Epsilon: 0.290000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 71 SimReward: 0.000001 Epsilon: 0.280000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 72 SimReward: 0.000043 Epsilon: 0.270000 TestReward: -1131.199707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.  1200.]]\n",
      "Epoch #: 73 SimReward: -0.000006 Epsilon: 0.260000 TestReward: 0.000000\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 74 SimReward: -0.000018 Epsilon: 0.250000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 75 SimReward: 0.000050 Epsilon: 0.240000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 76 SimReward: 0.000045 Epsilon: 0.230000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 77 SimReward: 0.000014 Epsilon: 0.220000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 78 SimReward: 0.000097 Epsilon: 0.210000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 79 SimReward: -0.000020 Epsilon: 0.200000 TestReward: 1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 80 SimReward: -0.000021 Epsilon: 0.190000 TestReward: 0.000000\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 81 SimReward: -0.000023 Epsilon: 0.180000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 82 SimReward: -0.000040 Epsilon: 0.170000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 83 SimReward: -0.000012 Epsilon: 0.160000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 84 SimReward: 0.000081 Epsilon: 0.150000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 85 SimReward: 0.000002 Epsilon: 0.140000 TestReward: 0.000000\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 86 SimReward: -0.000066 Epsilon: 0.130000 TestReward: -1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 87 SimReward: 0.000040 Epsilon: 0.120000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 88 SimReward: 0.000059 Epsilon: 0.110000 TestReward: 1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 89 SimReward: -0.000021 Epsilon: 0.100000 TestReward: 0.000000\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 90 SimReward: 0.000004 Epsilon: 0.100000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 91 SimReward: -0.000029 Epsilon: 0.100000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 92 SimReward: 0.000039 Epsilon: 0.100000 TestReward: 1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 93 SimReward: -0.000027 Epsilon: 0.100000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 94 SimReward: 0.000096 Epsilon: 0.100000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 95 SimReward: -0.000085 Epsilon: 0.100000 TestReward: -1131.199707\n",
      "[[   -1.  1199.]\n",
      " [    0.     1.]]\n",
      "Epoch #: 96 SimReward: 0.000054 Epsilon: 0.100000 TestReward: -1131.199707\n",
      "[[    0.  1200.]]\n",
      "Epoch #: 97 SimReward: 0.000122 Epsilon: 0.100000 TestReward: 0.000000\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 98 SimReward: -0.000044 Epsilon: 0.100000 TestReward: 1131.199707\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n",
      "Epoch #: 99 SimReward: -0.000009 Epsilon: 0.100000 TestReward: 1131.199707\n",
      "Completed in 33583.890000\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# set train and test data\n",
    "train_data = df_train\n",
    "test_data = df_test\n",
    "start_timestep = 14\n",
    "\n",
    "# training arguments\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_sim = 924 \n",
    "gamma = 0\n",
    "epsilon = 0.99  # parameter setting initial random actions, that decays as learning \n",
    "\n",
    "# stores tuples of (S, A, R, S')\n",
    "signal = pd.Series(index=np.arange(len(train_data)), data=[None]*len(train_data))\n",
    "\n",
    "# replay memory\n",
    "replay_memory = ReplayMemory(10000)\n",
    "\n",
    "# let the agent learn\n",
    "learning_progress = []\n",
    "sim_count = 0\n",
    "for i in range(epochs):\n",
    "    avg_sim_reward = AverageMeter()\n",
    "    terminal_state = 0\n",
    "    states, price_data = create_states(train_data)\n",
    "    time_step = start_timestep\n",
    "    state = states[time_step][None, :]\n",
    "        \n",
    "    #while game still in progress\n",
    "    while(terminal_state == 0):\n",
    "        \n",
    "        # We are in state S\n",
    "        # 1- Compute the next action based on state S with the network\n",
    "        if (random.random() < epsilon): #choose random action\n",
    "            action = np.random.randint(0,NUM_ACTIONS) #assumes 4 different actions\n",
    "        else: #choose best action from Q(s,a) values\n",
    "            qval = model.predict(state, batch_size=1)\n",
    "            action = (np.argmax(qval))\n",
    "            \n",
    "        # 2- Take action, observe new state S'\n",
    "        new_state, new_time_step, signal, terminal_state = take_action(states, action, signal, time_step)\n",
    "        assert new_time_step - time_step == 1\n",
    "       \n",
    "        # 3- Compute the reward\n",
    "        reward = get_pos_reward(new_state, new_time_step, action, price_data, signal, eval=False)\n",
    "#         print(\" Action: {}, Signal: {}, Reward: {} Price: {} TimeStep: {}\".format(action, \n",
    "#                                                                                   signal[time_step],\n",
    "#                                                                                   reward,\n",
    "#                                                                                   price_data[new_time_step],\n",
    "#                                                                                   time_step))\n",
    "        \n",
    "        # Update average simulation reward\n",
    "        avg_sim_reward.update(reward)\n",
    "       \n",
    "        # Add new experience to replay memory\n",
    "        replay_memory.push(state, action, new_state, reward)\n",
    "        \n",
    "        # Experience replay storage, if we add enough new experiences\n",
    "        optimize_DQN(model, replay_memory, batch_size, gamma)\n",
    "        \n",
    "        # One step ahead\n",
    "        state = new_state\n",
    "        time_step = new_time_step\n",
    "        assert (states[new_time_step] - new_state).sum() == 0\n",
    "        \n",
    "    # eval model on test data\n",
    "    test_reward, action_count = evaluate_Q(test_data, model, get_pos_reward, )\n",
    "    print(action_count)\n",
    "    # learning_progress.append((eval_reward))\n",
    "    learning_progress.append(avg_sim_reward.avg)\n",
    "    print(\"Epoch #: %s SimReward: %f Epsilon: %f TestReward: %f\" % (i, avg_sim_reward.avg, epsilon, test_reward))\n",
    "    \n",
    "    # decay epsilon\n",
    "    if epsilon > 0.1: #decrement epsilon over time\n",
    "        epsilon -= (1.0/epochs)\n",
    "\n",
    "elapsed = np.round(timeit.default_timer() - start_time, decimals=2)\n",
    "print(\"Completed in %f\" % (elapsed,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131.19970703\n",
      "[[    0.     1.]\n",
      " [    1.  1199.]]\n"
     ]
    }
   ],
   "source": [
    "reward, action_count = evaluate_Q(test_data, model, get_pos_reward, )\n",
    "print(reward)\n",
    "print(action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, price_data = create_states(test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal = pd.Series(index=np.arange(len(test_data)))\n",
    "state, xdata, price_data = create_states(test_data, test=True)\n",
    "\n",
    "time_step = 1\n",
    "terminal_state = 0\n",
    "avg_test_reward = AverageMeter()\n",
    "while(terminal_state == 0):\n",
    "    #We are in state S\n",
    "    # 1- Compute the next action based on state S with the network\n",
    "    qval = model.predict(state, batch_size=1)\n",
    "    print(qval)\n",
    "    action = (np.argmax(qval))\n",
    "    # 2- Take action, observe new state S'\n",
    "    new_state, time_step, signal, terminal_state = take_action(state, xdata, action, signal, time_step)\n",
    "        \n",
    "    # 3- Compute the reward\n",
    "#     reward = get_reward(new_state, time_step, action, close_prices, signal, eval=False)\n",
    "#     avg_test_reward.update(reward)\n",
    "    state = new_state\n",
    "#     print(\" # Average test reward: {}\".format(avg_test_reward.avg))\n",
    "    \n",
    "final_reward = get_pos_reward(new_state, time_step, action, price_data, signal, eval=True)\n",
    "print(\" # Final test reward: {}\".format(final_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_data = pd.Series(price_data)\n",
    "bt = twp.Backtest(price_data, signal, signalType='shares')\n",
    "bt.data['delta'] = bt.data['shares'].diff().fillna(0)\n",
    "\n",
    "unique, counts = np.unique(filter(lambda v: v==v, signal.values), return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20, 40])\n",
    "plt.subplot(3,1,1)\n",
    "bt.plotTrades()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "bt.pnl.plot(style='x-')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(learning_progress)\n",
    "\n",
    "plt.savefig('reinforcement_learning/plt/summary'+'.png', bbox_inches='tight', pad_inches=1, dpi=72)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "894px",
    "left": "1670px",
    "right": "20.2333px",
    "top": "121px",
    "width": "454px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
